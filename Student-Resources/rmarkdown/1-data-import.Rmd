---
title: "Creating Spark Sessions"
output: 
  html_notebook:
    toc: true
    toc_depth: 2
    toc_float: true
editor_options: 
  chunk_output_type: inline
---

# Create Spark Context

In this session we will create a Spark Session using the `rxSparkConnect` function from the `RevoScaleR` package. This will a `scaleR` Spark Session and create some additional necessary paths for us. We can use the `RxHiveData` method to point to data that is in our Hive metastore. Moreover, we can run MRS functions directly against the Hive data.

```{r spark_context}

# Sys.setenv(SPARK_HOME = "/usr/hdp/current/spark2-client")
cc <- rxSparkConnect()
hive_tbl <- RxHiveData(table = "hivesampletable",
                       colInfo = list(clientid = list(type = "integer"),
                                      querytime = list(type = "character"),
                                      market = list(type = "factor"),
                                      deviceplatform = list(type = "factor")))
rxGetInfo(hive_tbl, getVarInfo = TRUE)
rxSummary(~querydwelltime, data = hive_tbl)
rxSparkDisconnect()
```


## Adding `sparklyr` interoperability

We can next use the `interop` parameter to create a Spark session that can be shared by `sparklyr`.

```{r interop}

options(rsparkling.sparklingwater.version = "2.1.1")
library(rsparkling)
library(sparklyr)
library(dplyr)
cc <- rxSparkConnect(interop = "sparklyr",
                     extraSparkConfig = "--conf spark.dynamicAllocation.enabled=false",
                     consoleOutput = TRUE)
sc <- rxGetSparklyrConnection(cc)

src_tbls(sc)
sample_tbl <- tbl(sc, "hivesampletable")

```

Now we can use our favorite `dplyr` methods with Spark SQL to query our Spark DataFrames and Hive catalog.

```{r hive_query}
ave_dwell <- sample_tbl %>% 
  group_by(deviceplatform) %>% 
  summarise(avedwell = mean(querydwelltime),
            numclients = n_distinct(clientid))
ave_dwell %>% collect
```



# Download Sample Data 

```{r download_data}

# download.file("https://alizaidi.blob.core.windows.net/training/sample_taxi.csv", "sample_taxi.csv")
# or the larger data!
download.file("http://alizaidi.blob.core.windows.net/training/taxi_large.csv", "taxi_large.csv")

wasb_taxi <- "/NYCTaxi/sample"
rxHadoopListFiles("/")
rxHadoopMakeDir(wasb_taxi)
rxHadoopCopyFromLocal("taxi_large.csv", wasb_taxi)
rxHadoopCommand("fs -cat /NYCTaxi/sample/taxi_large.csv | head")


```

Let's also download the sample data that we can work with locally.

```{r download_sample}
taxi_url <- "http://alizaidi.blob.core.windows.net/training/trainingData/manhattan_df.rds"
taxi_df  <- readRDS(gzcon(url(taxi_url)))
(taxi_df <- tbl_df(taxi_df))
```


# Import Data

To import data from csv files, we can use the `spark_read_csv` function, which is basically a wrapper for the `read.df` function using the __databricks.spark.csv__ package.

```{r import_csv}

taxi <- spark_read_csv(sc,
                       path = wasb_taxi,
                       "taxisample",
                       header = TRUE)

```

